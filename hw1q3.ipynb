{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw1q3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gsc16/MachineLearning/blob/master/hw1q3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "nmhTVRGsVBh_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Imports"
      ]
    },
    {
      "metadata": {
        "id": "3UFi5pw44gHF",
        "colab_type": "code",
        "outputId": "b7e3524d-74f2-4bb5-8de0-fa1542280134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "import math"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "k1dze9kK4fSc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading images from MNIST dataset"
      ]
    },
    {
      "metadata": {
        "id": "u67qvbgpU7cm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(train_images_original, train_labels_original), (test_images_original, test_labels_original) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kt9zW_28WeIw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Displaying the original values"
      ]
    },
    {
      "metadata": {
        "id": "jXW4rjttWjmh",
        "colab_type": "code",
        "outputId": "7d3ca4eb-f27d-4f05-e7a4-7950e933f944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "cell_type": "code",
      "source": [
        "train_images_original.shape, train_labels_original, test_images_original.shape, test_labels_original.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28),\n",
              " array([5, 0, 4, ..., 5, 6, 8], dtype=uint8),\n",
              " (10000, 28, 28),\n",
              " (10000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "gxGF4kP5WTSq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Extracting images - Flattening and Reshaping"
      ]
    },
    {
      "metadata": {
        "id": "Vy8UC4a4WS22",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images_new = train_images_original.reshape(60000, 28*28)\n",
        "train_images_new = train_images_new.astype('float32') / 255\n",
        "train_images_new = train_images_new.T\n",
        "\n",
        "test_images_new = test_images_original.reshape(10000, 28*28)\n",
        "test_images_new = test_images_new.astype('float32') / 255\n",
        "test_images_new = test_images_new.T\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xjVXxQ7l43rO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Converting integer vectors to binary"
      ]
    },
    {
      "metadata": {
        "id": "PHZayi89Fgc0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to convert integer values of labels to binary\n",
        "def convert_to_binary(train_labels_original):\n",
        "    op = []\n",
        "    for i in train_labels_original:\n",
        "      t = np.zeros(10) # 10 is the number of classifiers\n",
        "      t[i] = 1\n",
        "      op.append(t)\n",
        "    op = np.array(op)\n",
        "    op = op.reshape(train_labels_original.shape[0],10)\n",
        "    return op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ftLmk4m649nN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_labels_new = convert_to_binary(train_labels_original)\n",
        "test_labels_new = convert_to_binary(test_labels_original)\n",
        "train_labels_new = train_labels_new.T\n",
        "test_labels_new = test_labels_new.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qc2MNy5SWrW-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Displaying new values"
      ]
    },
    {
      "metadata": {
        "id": "UM7GoBRgWtR9",
        "colab_type": "code",
        "outputId": "b341577c-31f0-49a1-8cf7-8e5ea50d2189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_images_new.shape, train_labels_new.shape, test_images_new.shape, test_labels_new.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((784, 60000), (10, 60000), (784, 10000), (10, 10000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "GPx2Gsm-ipUc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Initializing the learning rate, batch size and number of epochs"
      ]
    },
    {
      "metadata": {
        "id": "Pk6ULRdiiuHg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Voy20j8Th4iU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Definitions"
      ]
    },
    {
      "metadata": {
        "id": "nIo1B1ZZh6Yc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Function to calculate logistic regression\n",
        "def log_reg(w,b,X):\n",
        "  return np.dot(w, X) + b\n",
        "\n",
        "#Function to calculate the value of softmax function\n",
        "def softmax(x): \n",
        "  e_x = np.exp(x - np.max(x))\n",
        "  return e_x/e_x.sum(axis=0)\n",
        "\n",
        "# Function to calculate the mini batches\n",
        "def cal_mini_batches(X, Y):\n",
        "    for index in range(0, X.shape[0] - batch_size + 1, batch_size):\n",
        "        new_batch = slice(index, index + batch_size)\n",
        "        yield X[new_batch], Y[new_batch]\n",
        "        \n",
        "# Function to calculate cross entropy loss\n",
        "def cross_entropy(Y, Y_Calc):\n",
        "    return -np.mean(Y * np.log(Y_Calc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GQP4qvbo8ANL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training the classifiers"
      ]
    },
    {
      "metadata": {
        "id": "IaSda0dWnK-W",
        "colab_type": "code",
        "outputId": "f2da7418-6bcf-4235-dffe-7fcb74f99259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "#initialization of weights\n",
        "w = np.random.randn(train_labels_new.shape[0], train_images_new.shape[0]) * 0.01\n",
        "b = np.zeros(shape=(train_labels_new.shape[0], 1))\n",
        "  \n",
        "#training\n",
        "for epoch in range(epochs):\n",
        "     \n",
        "  #splitting the training and test  set according to bacth sizes\n",
        "  for batch in cal_mini_batches(train_images_new.T, train_labels_new.T):\n",
        " \n",
        "    x_batch, y_batch = batch\n",
        "    xt = x_batch.T\n",
        "    yt = y_batch.T\n",
        "    m = xt.shape[0]\n",
        "        \n",
        "    #forward propogation\n",
        "    Y = log_reg(w,b,xt)\n",
        "    Z = softmax(Y);\n",
        "\n",
        "    cost = cross_entropy(yt, Z)\n",
        "        \n",
        "    #back propogation - gradient descent\n",
        "    loss = Z-yt\n",
        "\n",
        "    gradient_w = (1/m) * np.dot(loss, xt.T)\n",
        "    gradient_b = (1/m) * np.sum(loss, axis=1, keepdims=True)\n",
        "\n",
        "    w = w - learning_rate * gradient_w\n",
        "    b = b - learning_rate * gradient_b\n",
        "    \n",
        "\n",
        "# Creating one hot encoding of the softmax ouptut to calculate the accuracy\n",
        "y_train = log_reg(w,b,train_images_new)\n",
        "z_train = softmax(y_train)\n",
        "z_train  =  z_train.T\n",
        "\n",
        "y_test = log_reg(w,b,test_images_new)\n",
        "z_test = softmax(y_test)\n",
        "z_test  =  z_test.T\n",
        "  \n",
        "train_labels_new = train_labels_new.T\n",
        "test_labels_new  = test_labels_new.T\n",
        "  \n",
        "train_labels_prediction = np.zeros_like(train_labels_new)\n",
        "train_labels_prediction[np.arange(len(z_train)), z_train.argmax(1)] = 1\n",
        "  \n",
        "test_labels_prediction = np.zeros_like(test_labels_new)\n",
        "test_labels_prediction[np.arange(len(z_test)), z_test.argmax(1)] = 1\n",
        "  \n",
        "  \n",
        "# Calculating training and testing accuracies\n",
        "train_acc = 100 - np.mean(np.abs(train_labels_prediction - train_labels_new)) * 100\n",
        "train_acc = np.round(train_acc,2)\n",
        "test_acc = 100 - np.mean(np.abs(test_labels_prediction - test_labels_new)) * 100\n",
        "test_acc = np.round(test_acc,2)\n",
        "print(\"Training accuracy is \"+ str(train_acc)+\" %\")\n",
        "print(\"Testing accuracy is \"+str(test_acc)+\" %\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy is 95.47 %\n",
            "Testing accuracy is 95.65 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}